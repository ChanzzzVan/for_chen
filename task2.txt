2. **模型微调与调试：**
   - 假设你正在使用一个预训练的大模型进行特定领域的文本生成任务，但在初步测试中发现模型对某些关键词的理解不足。请设计一个微调策略，并展示如何通过添加特定数据集和调整训练配置来改进模型的表现。
答：
1. 针对理解不足的关键词，收集高质量的数据集(已有文档，爬网站)，准备输入输出对（包括：文本生成、填空等）
2. 用prompt template改写（这里要注意instruction上要写明任务类型：文本生成、填空等）
3. SFT
	- 使用LoRa，只调整旁路的参数
	- 训练模型
5. RLHF
	- 奖励模型：
   		生成的由人工来评价
   		填空的可以由和标准答案的相似度来评价
   		如有其他维度，考虑自动评价指标
	- 算法：PPO
